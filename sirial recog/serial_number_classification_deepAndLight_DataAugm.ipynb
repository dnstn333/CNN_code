{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 깊은 학습\n",
    "* 그리고 가벼움\n",
    "* 간단한 갯수를 돌림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8596\n",
      "6017 + 1289 + 1290 = 8596\n",
      "(6017, 28, 28, 1)\n",
      "(6017, 10)\n",
      "(1290, 28, 28, 1)\n",
      "(1290, 10)\n",
      "...................................................................................................\n",
      "100 : ...................................................................................................\n",
      "200 : ...................................................................................................\n",
      "300 : ...................................................................................................\n",
      "400 : ...................................................................................................\n",
      "500 : .(15970, 28, 28, 1)\n",
      "(15970, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import cv2\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "dataset_dir = \"../dataset(revision)\"\n",
    "\n",
    "def shuffle_data(data, label):\n",
    "    idx = np.arange(len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    shuffled_data = np.array(data)[np.array(idx)]\n",
    "    shuffled_label = np.array(label)[np.array(idx)]\n",
    "    \n",
    "    return shuffled_data, shuffled_label, idx\n",
    "\n",
    "\n",
    "def load_data(parent_dir, ext):\n",
    "    filepath_list = []\n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    dirnames = os.listdir(parent_dir)\n",
    "    for dirname in dirnames: # 하위 디렉토리 탐색\n",
    "        subdirpath = os.path.join(parent_dir, dirname)\n",
    "        if os.path.isdir(subdirpath): # 디렉토리인 경우\n",
    "            filenames = os.listdir(subdirpath)\n",
    "        \n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(subdirpath, filename)\n",
    "            \n",
    "                _ext = os.path.splitext(filepath)[-1] # 파일 확장자 확인\n",
    "                if _ext == ext:\n",
    "                    image = mpimg.imread(filepath)\n",
    "                    if image is None:\n",
    "                        continue\n",
    "                    \n",
    "                    image = image.astype(np.float32) / 255.\n",
    "                    image = image.reshape(28, 28, 1)\n",
    "\n",
    "                    image_list.append(image)\n",
    "                    label_list.append(dirname)\n",
    "                    filepath_list.append(filepath)\n",
    "                    \n",
    "    return image_list, label_list, filepath_list\n",
    "\n",
    "\n",
    "# data 불러오기\n",
    "image_list, label_list, filepath_list = load_data(dataset_dir, '.bmp')\n",
    "\n",
    "# one-hot vector로 변환\n",
    "label_list = np_utils.to_categorical(label_list, num_classes)\n",
    "\n",
    "\n",
    "# dataset 분할\n",
    "data_len = len(label_list)\n",
    "train_size = int(0.7 * data_len)\n",
    "valid_size = int(0.15 * data_len)\n",
    "test_size = data_len - train_size - valid_size\n",
    "print(data_len)\n",
    "print(train_size, '+', valid_size, '+', test_size, '=', train_size+valid_size+test_size)\n",
    "\n",
    "\n",
    "# data shuffling\n",
    "shuffled_image_list, shuffled_label_list, shuffled_idx = shuffle_data(image_list, label_list)\n",
    "\n",
    "# dataset 구축\n",
    "x_train = shuffled_image_list[:train_size]\n",
    "y_train = shuffled_label_list[:train_size]\n",
    "x_valid = shuffled_image_list[train_size:data_len-test_size]\n",
    "y_valid = shuffled_label_list[train_size:data_len-test_size]\n",
    "x_test = shuffled_image_list[data_len-test_size:]\n",
    "y_test = shuffled_label_list[data_len-test_size:]\n",
    "shuffle_indices = shuffled_idx[data_len-test_size:] # 디버그용\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "### Start about Image Data Augmented ###\n",
    "\n",
    "# define data preparation\n",
    "datagen = image.ImageDataGenerator(width_shift_range=0.2, height_shift_range=0.05, fill_mode='constant', cval=0.25)\n",
    "\n",
    "# fit parameters from data\n",
    "x_trainAugm = np.empty((0,) + x_train.shape[1:])\n",
    "y_trainAugm = np.empty((0,) + y_train.shape[1:])\n",
    "cnt = 0\n",
    "for x_augm, y_augm in datagen.flow(x_train, y_train):\n",
    "    cnt+=1\n",
    "    x_trainAugm = np.concatenate((x_trainAugm, x_augm))\n",
    "    y_trainAugm = np.concatenate((y_trainAugm, y_augm))\n",
    "    if cnt%100==0:\n",
    "        print('')\n",
    "        print(str(cnt)+' :',end=' ')\n",
    "    else:\n",
    "        print('.',end='')\n",
    "        \n",
    "    if cnt > 500:\n",
    "        break\n",
    "\n",
    "print(x_trainAugm.shape)\n",
    "print(y_trainAugm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15970, 28, 28, 1)\n",
      "(15970, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_trainAugm.shape)\n",
    "print(y_trainAugm.shape)\n",
    "\n",
    "x_train = x_trainAugm\n",
    "y_train = y_trainAugm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 16)          2320      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 16)          2320      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 51,520\n",
      "Trainable params: 51,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 15970 samples, validate on 1289 samples\n",
      "Epoch 1/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 1.7159 - acc: 0.3945Epoch 00000: loss improved from inf to 1.70827, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 8s - loss: 1.7083 - acc: 0.3973 - val_loss: 0.0742 - val_acc: 0.9907\n",
      "Epoch 2/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.2615 - acc: 0.9138Epoch 00001: loss improved from 1.70827 to 0.26109, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 6s - loss: 0.2611 - acc: 0.9140 - val_loss: 0.0152 - val_acc: 0.9961\n",
      "Epoch 3/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9608Epoch 00002: loss improved from 0.26109 to 0.12687, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 6s - loss: 0.1269 - acc: 0.9609 - val_loss: 0.0076 - val_acc: 0.9969\n",
      "Epoch 4/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9747Epoch 00003: loss improved from 0.12687 to 0.07921, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 6s - loss: 0.0792 - acc: 0.9747 - val_loss: 0.0117 - val_acc: 0.9969\n",
      "Epoch 5/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9789Epoch 00004: loss improved from 0.07921 to 0.06331, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 6s - loss: 0.0633 - acc: 0.9788 - val_loss: 0.0063 - val_acc: 0.9977\n",
      "Epoch 6/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9826Epoch 00005: loss improved from 0.06331 to 0.05454, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 6s - loss: 0.0545 - acc: 0.9826 - val_loss: 0.0098 - val_acc: 0.9984\n",
      "Epoch 7/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9855Epoch 00006: loss improved from 0.05454 to 0.04411, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 6s - loss: 0.0441 - acc: 0.9855 - val_loss: 0.0120 - val_acc: 0.9977\n",
      "Epoch 8/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9868Epoch 00007: loss improved from 0.04411 to 0.03897, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 6s - loss: 0.0390 - acc: 0.9869 - val_loss: 0.0072 - val_acc: 0.9984\n",
      "Epoch 9/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9879Epoch 00008: loss improved from 0.03897 to 0.03546, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 6s - loss: 0.0355 - acc: 0.9879 - val_loss: 0.0017 - val_acc: 0.9984\n",
      "Epoch 10/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9877Epoch 00009: loss improved from 0.03546 to 0.03515, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 6s - loss: 0.0352 - acc: 0.9877 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 11/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9899Epoch 00010: loss improved from 0.03515 to 0.02866, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 6s - loss: 0.0287 - acc: 0.9899 - val_loss: 0.0066 - val_acc: 0.9984\n",
      "Epoch 12/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9903Epoch 00011: loss did not improve\n",
      "15970/15970 [==============================] - 6s - loss: 0.0290 - acc: 0.9904 - val_loss: 0.0025 - val_acc: 0.9984\n",
      "Epoch 13/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9902Epoch 00012: loss improved from 0.02866 to 0.02744, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 6s - loss: 0.0274 - acc: 0.9902 - val_loss: 0.0020 - val_acc: 0.9992\n",
      "Epoch 14/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9923Epoch 00013: loss improved from 0.02744 to 0.02237, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 6s - loss: 0.0224 - acc: 0.9922 - val_loss: 0.0025 - val_acc: 0.9984\n",
      "Epoch 15/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9904Epoch 00014: loss did not improve\n",
      "15970/15970 [==============================] - 6s - loss: 0.0248 - acc: 0.9904 - val_loss: 0.0032 - val_acc: 0.9984\n",
      "Epoch 16/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9926Epoch 00015: loss improved from 0.02237 to 0.02140, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0214 - acc: 0.9926 - val_loss: 0.0029 - val_acc: 0.9984\n",
      "Epoch 17/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9933Epoch 00016: loss improved from 0.02140 to 0.02019, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0202 - acc: 0.9932 - val_loss: 0.0031 - val_acc: 0.9984\n",
      "Epoch 18/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9938Epoch 00017: loss improved from 0.02019 to 0.01780, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0178 - acc: 0.9938 - val_loss: 0.0016 - val_acc: 0.9984\n",
      "Epoch 19/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9934Epoch 00018: loss improved from 0.01780 to 0.01746, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0175 - acc: 0.9935 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 20/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9934Epoch 00019: loss improved from 0.01746 to 0.01641, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0164 - acc: 0.9935 - val_loss: 0.0024 - val_acc: 0.9984\n",
      "Epoch 21/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9939Epoch 00020: loss improved from 0.01641 to 0.01638, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0164 - acc: 0.9939 - val_loss: 0.0059 - val_acc: 0.9984\n",
      "Epoch 22/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9942Epoch 00021: loss improved from 0.01638 to 0.01612, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0161 - acc: 0.9942 - val_loss: 0.0071 - val_acc: 0.9984\n",
      "Epoch 23/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9923Epoch 00022: loss did not improve\n",
      "15970/15970 [==============================] - 7s - loss: 0.0188 - acc: 0.9924 - val_loss: 7.0643e-04 - val_acc: 1.0000\n",
      "Epoch 24/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9952Epoch 00023: loss improved from 0.01612 to 0.01341, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0134 - acc: 0.9952 - val_loss: 0.0015 - val_acc: 0.9992\n",
      "Epoch 25/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9937Epoch 00024: loss did not improve\n",
      "15970/15970 [==============================] - 6s - loss: 0.0157 - acc: 0.9937 - val_loss: 0.0011 - val_acc: 0.9992\n",
      "Epoch 26/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9954Epoch 00025: loss improved from 0.01341 to 0.01258, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0126 - acc: 0.9954 - val_loss: 5.0951e-04 - val_acc: 1.0000\n",
      "Epoch 27/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9958Epoch 00026: loss improved from 0.01258 to 0.01170, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 6s - loss: 0.0117 - acc: 0.9958 - val_loss: 9.2540e-04 - val_acc: 1.0000\n",
      "Epoch 28/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9949Epoch 00027: loss did not improve\n",
      "15970/15970 [==============================] - 6s - loss: 0.0132 - acc: 0.9949 - val_loss: 7.5133e-05 - val_acc: 1.0000\n",
      "Epoch 29/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9953Epoch 00028: loss did not improve\n",
      "15970/15970 [==============================] - 6s - loss: 0.0126 - acc: 0.9954 - val_loss: 3.3005e-04 - val_acc: 1.0000\n",
      "Epoch 30/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9958Epoch 00029: loss improved from 0.01170 to 0.01113, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0111 - acc: 0.9959 - val_loss: 0.0030 - val_acc: 0.9984\n",
      "Epoch 31/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9951Epoch 00030: loss did not improve\n",
      "15970/15970 [==============================] - 6s - loss: 0.0124 - acc: 0.9951 - val_loss: 2.9482e-04 - val_acc: 1.0000\n",
      "Epoch 32/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9948Epoch 00031: loss did not improve\n",
      "15970/15970 [==============================] - 6s - loss: 0.0142 - acc: 0.9948 - val_loss: 2.2691e-04 - val_acc: 1.0000\n",
      "Epoch 33/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9951Epoch 00032: loss did not improve\n",
      "15970/15970 [==============================] - 6s - loss: 0.0117 - acc: 0.9951 - val_loss: 2.8963e-04 - val_acc: 1.0000\n",
      "Epoch 34/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9963Epoch 00033: loss improved from 0.01113 to 0.01038, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0104 - acc: 0.9963 - val_loss: 4.4054e-05 - val_acc: 1.0000\n",
      "Epoch 35/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9958Epoch 00034: loss did not improve\n",
      "15970/15970 [==============================] - 7s - loss: 0.0111 - acc: 0.9959 - val_loss: 2.2066e-05 - val_acc: 1.0000\n",
      "Epoch 36/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9953Epoch 00035: loss did not improve\n",
      "15970/15970 [==============================] - 7s - loss: 0.0113 - acc: 0.9953 - val_loss: 4.2549e-05 - val_acc: 1.0000\n",
      "Epoch 37/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9966Epoch 00036: loss improved from 0.01038 to 0.00856, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0086 - acc: 0.9966 - val_loss: 1.0119e-04 - val_acc: 1.0000\n",
      "Epoch 38/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9963Epoch 00037: loss did not improve\n",
      "15970/15970 [==============================] - 7s - loss: 0.0109 - acc: 0.9963 - val_loss: 2.1610e-04 - val_acc: 1.0000\n",
      "Epoch 39/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9963Epoch 00038: loss did not improve\n",
      "15970/15970 [==============================] - 6s - loss: 0.0098 - acc: 0.9963 - val_loss: 1.2656e-04 - val_acc: 1.0000\n",
      "Epoch 40/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9972Epoch 00039: loss improved from 0.00856 to 0.00842, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0084 - acc: 0.9972 - val_loss: 5.2096e-04 - val_acc: 1.0000\n",
      "Epoch 41/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9966Epoch 00040: loss improved from 0.00842 to 0.00842, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0084 - acc: 0.9966 - val_loss: 2.0264e-04 - val_acc: 1.0000\n",
      "Epoch 42/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9960Epoch 00041: loss did not improve\n",
      "15970/15970 [==============================] - 6s - loss: 0.0093 - acc: 0.9961 - val_loss: 7.9410e-05 - val_acc: 1.0000\n",
      "Epoch 43/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9975Epoch 00042: loss improved from 0.00842 to 0.00698, saving model to ./logs/weights.best_deepAndLight_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 7s - loss: 0.0070 - acc: 0.9975 - val_loss: 2.1437e-04 - val_acc: 1.0000\n",
      "Epoch 44/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9966Epoch 00043: loss did not improve\n",
      "15970/15970 [==============================] - 6s - loss: 0.0099 - acc: 0.9966 - val_loss: 8.8953e-05 - val_acc: 1.0000\n",
      "Epoch 45/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9963Epoch 00044: loss did not improve\n",
      "15970/15970 [==============================] - 6s - loss: 0.0097 - acc: 0.9963 - val_loss: 9.3875e-05 - val_acc: 1.0000\n",
      "Epoch 46/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9963Epoch 00045: loss did not improve\n",
      "15970/15970 [==============================] - 6s - loss: 0.0103 - acc: 0.9963 - val_loss: 7.4226e-05 - val_acc: 1.0000\n",
      "Test loss:  9.89412681686e-07\n",
      "Test accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Activation\n",
    "from keras.models import Sequential, save_model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters= 16, kernel_size = (3,3), activation='relu', padding='same', input_shape= x_train.shape[1:]))\n",
    "model.add(Conv2D(filters= 16, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size= (2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters= 16, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(filters= 16, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size= (2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters= 16, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(filters= 16, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# configure the model\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# prints a summary representation of the model\n",
    "model.summary()\n",
    "\n",
    "# set tensorboard\n",
    "from keras.callbacks import TensorBoard\n",
    "tbCallback = TensorBoard(log_dir='./tensorboard/deepAndLightDataAugm')\n",
    "tbCallback.set_model(model)\n",
    "\n",
    "# save the model\n",
    "save_model(model, './logs/my_model_deepAndLight_DataAugm.hdf5')\n",
    "\n",
    "# make parameters checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(filepath=\"./logs/weights.best_deepAndLight_DataAugm.hdf5\", verbose=1, \n",
    "                               monitor='loss', save_best_only=True, mode='auto')\n",
    "\n",
    "# # set early stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=False, # previously shuffled\n",
    "                validation_data=(x_valid, y_valid),\n",
    "                callbacks = [tbCallback, checkpoint, early_stopping])\n",
    "\n",
    "'''\n",
    "learning is done at this point\n",
    "'''\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss: ', score[0])\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 16)          2320      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 16)          2320      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 51,520\n",
      "Trainable params: 51,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "incorrect:  0 , total:  1290  accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# restore trained model\n",
    "loaded_model = load_model('./logs/my_model_deepAndLight_DataAugm.hdf5')\n",
    "loaded_model.load_weights('./logs/weights.best_deepAndLight_DataAugm.hdf5')\n",
    "\n",
    "# prints a summary representation of the model\n",
    "loaded_model.summary()\n",
    "\n",
    "# predict test data\n",
    "predict = loaded_model.predict(x_test)\n",
    "\n",
    "# show error cases\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "cnt = 0\n",
    "for i in range(test_size):\n",
    "    if not (np.argmax(predict[i]) == np.argmax(y_test[i])):\n",
    "        print('file: %s' % filepath_list[shuffle_indices[i]])\n",
    "        print(np.argmax(predict[i]))\n",
    "        cnt += 1\n",
    "\n",
    "        img = mpimg.imread(filepath_list[shuffle_indices[i]])\n",
    "\n",
    "        plt.gray()\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "print('incorrect: ', cnt, ', total: ', test_size, ' accuracy: ', (test_size-cnt)/test_size)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
