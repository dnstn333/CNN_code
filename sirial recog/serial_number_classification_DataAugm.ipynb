{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. data preprocessing 및 dataset 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8596\n",
      "6017 + 1289 + 1290 = 8596\n",
      "(6017, 28, 28, 1)\n",
      "(6017, 10)\n",
      "(1290, 28, 28, 1)\n",
      "(1290, 10)\n",
      "...................................................................................................\n",
      "100 : ...................................................................................................\n",
      "200 : ...................................................................................................\n",
      "300 : ...................................................................................................\n",
      "400 : ...................................................................................................\n",
      "500 : .(15970, 28, 28, 1)\n",
      "(15970, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import cv2\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "dataset_dir = \"../dataset(revision)\"\n",
    "\n",
    "def shuffle_data(data, label):\n",
    "    idx = np.arange(len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    shuffled_data = np.array(data)[np.array(idx)]\n",
    "    shuffled_label = np.array(label)[np.array(idx)]\n",
    "    \n",
    "    return shuffled_data, shuffled_label, idx\n",
    "\n",
    "\n",
    "def load_data(parent_dir, ext):\n",
    "    filepath_list = []\n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    dirnames = os.listdir(parent_dir)\n",
    "    for dirname in dirnames: # 하위 디렉토리 탐색\n",
    "        subdirpath = os.path.join(parent_dir, dirname)\n",
    "        if os.path.isdir(subdirpath): # 디렉토리인 경우\n",
    "            filenames = os.listdir(subdirpath)\n",
    "        \n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(subdirpath, filename)\n",
    "            \n",
    "                _ext = os.path.splitext(filepath)[-1] # 파일 확장자 확인\n",
    "                if _ext == ext:\n",
    "                    image = mpimg.imread(filepath)\n",
    "                    if image is None:\n",
    "                        continue\n",
    "                    \n",
    "                    image = image.astype(np.float32) / 255.\n",
    "                    image = image.reshape(28, 28, 1)\n",
    "\n",
    "                    image_list.append(image)\n",
    "                    label_list.append(dirname)\n",
    "                    filepath_list.append(filepath)\n",
    "                    \n",
    "    return image_list, label_list, filepath_list\n",
    "\n",
    "\n",
    "# data 불러오기\n",
    "image_list, label_list, filepath_list = load_data(dataset_dir, '.bmp')\n",
    "\n",
    "# one-hot vector로 변환\n",
    "label_list = np_utils.to_categorical(label_list, num_classes)\n",
    "\n",
    "\n",
    "# dataset 분할\n",
    "data_len = len(label_list)\n",
    "train_size = int(0.7 * data_len)\n",
    "valid_size = int(0.15 * data_len)\n",
    "test_size = data_len - train_size - valid_size\n",
    "print(data_len)\n",
    "print(train_size, '+', valid_size, '+', test_size, '=', train_size+valid_size+test_size)\n",
    "\n",
    "\n",
    "# data shuffling\n",
    "shuffled_image_list, shuffled_label_list, shuffled_idx = shuffle_data(image_list, label_list)\n",
    "\n",
    "# dataset 구축\n",
    "x_train = shuffled_image_list[:train_size]\n",
    "y_train = shuffled_label_list[:train_size]\n",
    "x_valid = shuffled_image_list[train_size:data_len-test_size]\n",
    "y_valid = shuffled_label_list[train_size:data_len-test_size]\n",
    "x_test = shuffled_image_list[data_len-test_size:]\n",
    "y_test = shuffled_label_list[data_len-test_size:]\n",
    "shuffle_indices = shuffled_idx[data_len-test_size:] # 디버그용\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "### Start about Image Data Augmented ###\n",
    "\n",
    "# define data preparation\n",
    "datagen = image.ImageDataGenerator(width_shift_range=0.2, height_shift_range=0.05, fill_mode='constant', cval=0.25)\n",
    "\n",
    "# fit parameters from data\n",
    "x_trainAugm = np.empty((0,) + x_train.shape[1:])\n",
    "y_trainAugm = np.empty((0,) + y_train.shape[1:])\n",
    "cnt = 0\n",
    "for x_augm, y_augm in datagen.flow(x_train, y_train):\n",
    "    cnt+=1\n",
    "    x_trainAugm = np.concatenate((x_trainAugm, x_augm))\n",
    "    y_trainAugm = np.concatenate((y_trainAugm, y_augm))\n",
    "    if cnt%100==0:\n",
    "        print('')\n",
    "        print(str(cnt)+' :',end=' ')\n",
    "    else:\n",
    "        print('.',end='')\n",
    "        \n",
    "    if cnt > 500:\n",
    "        break\n",
    "\n",
    "print(x_trainAugm.shape)\n",
    "print(y_trainAugm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15970, 28, 28, 1)\n",
      "(15970, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_trainAugm.shape)\n",
    "print(y_trainAugm.shape)\n",
    "\n",
    "x_train = x_trainAugm\n",
    "y_train = y_trainAugm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. model 구축 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                156850    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 229,152\n",
      "Trainable params: 229,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 15970 samples, validate on 1289 samples\n",
      "Epoch 1/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 1.6657 - acc: 0.4263Epoch 00000: loss improved from inf to 1.65925, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 10s - loss: 1.6593 - acc: 0.4284 - val_loss: 0.2578 - val_acc: 0.9310\n",
      "Epoch 2/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9124Epoch 00001: loss improved from 1.65925 to 0.27271, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.2727 - acc: 0.9127 - val_loss: 0.0096 - val_acc: 0.9977\n",
      "Epoch 3/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9684Epoch 00002: loss improved from 0.27271 to 0.10228, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.1023 - acc: 0.9684 - val_loss: 0.0045 - val_acc: 0.9984\n",
      "Epoch 4/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9807Epoch 00003: loss improved from 0.10228 to 0.06561, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0656 - acc: 0.9807 - val_loss: 0.0042 - val_acc: 0.9984\n",
      "Epoch 5/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9853Epoch 00004: loss improved from 0.06561 to 0.04847, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0485 - acc: 0.9854 - val_loss: 0.0012 - val_acc: 0.9992\n",
      "Epoch 6/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9871Epoch 00005: loss improved from 0.04847 to 0.03913, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0391 - acc: 0.9872 - val_loss: 7.5273e-04 - val_acc: 0.9992\n",
      "Epoch 7/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9900Epoch 00006: loss improved from 0.03913 to 0.03261, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0326 - acc: 0.9900 - val_loss: 7.5563e-04 - val_acc: 1.0000\n",
      "Epoch 8/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9900Epoch 00007: loss improved from 0.03261 to 0.03002, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0300 - acc: 0.9900 - val_loss: 1.1522e-04 - val_acc: 1.0000\n",
      "Epoch 9/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9924Epoch 00008: loss improved from 0.03002 to 0.02201, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0220 - acc: 0.9924 - val_loss: 3.3708e-05 - val_acc: 1.0000\n",
      "Epoch 10/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9933Epoch 00009: loss improved from 0.02201 to 0.02012, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0201 - acc: 0.9934 - val_loss: 3.7986e-05 - val_acc: 1.0000\n",
      "Epoch 11/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9938Epoch 00010: loss improved from 0.02012 to 0.01826, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0183 - acc: 0.9938 - val_loss: 2.4703e-05 - val_acc: 1.0000\n",
      "Epoch 12/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9935Epoch 00011: loss improved from 0.01826 to 0.01748, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0175 - acc: 0.9935 - val_loss: 1.7255e-05 - val_acc: 1.0000\n",
      "Epoch 13/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9938Epoch 00012: loss improved from 0.01748 to 0.01652, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0165 - acc: 0.9939 - val_loss: 4.0729e-06 - val_acc: 1.0000\n",
      "Epoch 14/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9945Epoch 00013: loss improved from 0.01652 to 0.01536, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0154 - acc: 0.9945 - val_loss: 1.7439e-05 - val_acc: 1.0000\n",
      "Epoch 15/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9951Epoch 00014: loss improved from 0.01536 to 0.01397, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0140 - acc: 0.9952 - val_loss: 1.7859e-05 - val_acc: 1.0000\n",
      "Epoch 16/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9950Epoch 00015: loss improved from 0.01397 to 0.01305, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0130 - acc: 0.9951 - val_loss: 4.1793e-06 - val_acc: 1.0000\n",
      "Epoch 17/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9968Epoch 00016: loss improved from 0.01305 to 0.00971, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0097 - acc: 0.9968 - val_loss: 1.0791e-06 - val_acc: 1.0000\n",
      "Epoch 18/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9960Epoch 00017: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0106 - acc: 0.9959 - val_loss: 4.4251e-07 - val_acc: 1.0000\n",
      "Epoch 19/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9963Epoch 00018: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0102 - acc: 0.9964 - val_loss: 9.9455e-07 - val_acc: 1.0000\n",
      "Epoch 20/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9965Epoch 00019: loss improved from 0.00971 to 0.00934, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0093 - acc: 0.9965 - val_loss: 4.6771e-07 - val_acc: 1.0000\n",
      "Epoch 21/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9957Epoch 00020: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0110 - acc: 0.9957 - val_loss: 3.8604e-07 - val_acc: 1.0000\n",
      "Epoch 22/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9968Epoch 00021: loss improved from 0.00934 to 0.00833, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0083 - acc: 0.9969 - val_loss: 5.1787e-06 - val_acc: 1.0000\n",
      "Epoch 23/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9963Epoch 00022: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0096 - acc: 0.9963 - val_loss: 5.2252e-06 - val_acc: 1.0000\n",
      "Epoch 24/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9962Epoch 00023: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0087 - acc: 0.9962 - val_loss: 2.8703e-07 - val_acc: 1.0000\n",
      "Epoch 25/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9975Epoch 00024: loss improved from 0.00833 to 0.00744, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0074 - acc: 0.9975 - val_loss: 3.5516e-07 - val_acc: 1.0000\n",
      "Epoch 26/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9971Epoch 00025: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0086 - acc: 0.9971 - val_loss: 1.6550e-05 - val_acc: 1.0000\n",
      "Epoch 27/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9968Epoch 00026: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0090 - acc: 0.9969 - val_loss: 1.3539e-07 - val_acc: 1.0000\n",
      "Epoch 28/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9965Epoch 00027: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0100 - acc: 0.9965 - val_loss: 1.4469e-07 - val_acc: 1.0000\n",
      "Epoch 29/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9982Epoch 00028: loss improved from 0.00744 to 0.00602, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0060 - acc: 0.9981 - val_loss: 2.2488e-07 - val_acc: 1.0000\n",
      "Epoch 30/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9967Epoch 00029: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0091 - acc: 0.9967 - val_loss: 1.3613e-07 - val_acc: 1.0000\n",
      "Epoch 31/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9967Epoch 00030: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0079 - acc: 0.9967 - val_loss: 1.2217e-07 - val_acc: 1.0000\n",
      "Epoch 32/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9971Epoch 00031: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0071 - acc: 0.9971 - val_loss: 3.8802e-06 - val_acc: 1.0000\n",
      "Epoch 33/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9974Epoch 00032: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0081 - acc: 0.9974 - val_loss: 1.3230e-07 - val_acc: 1.0000\n",
      "Epoch 34/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9976Epoch 00033: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0065 - acc: 0.9976 - val_loss: 1.2665e-07 - val_acc: 1.0000\n",
      "Epoch 35/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9970Epoch 00034: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0074 - acc: 0.9971 - val_loss: 1.2138e-07 - val_acc: 1.0000\n",
      "Epoch 36/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9980Epoch 00035: loss improved from 0.00602 to 0.00555, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0055 - acc: 0.9980 - val_loss: 1.3239e-07 - val_acc: 1.0000\n",
      "Epoch 37/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9972Epoch 00036: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0067 - acc: 0.9972 - val_loss: 1.2000e-07 - val_acc: 1.0000\n",
      "Epoch 38/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9977Epoch 00037: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0062 - acc: 0.9977 - val_loss: 1.1995e-07 - val_acc: 1.0000\n",
      "Epoch 39/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9974Epoch 00038: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0060 - acc: 0.9974 - val_loss: 2.1003e-07 - val_acc: 1.0000\n",
      "Epoch 40/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9979Epoch 00039: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0061 - acc: 0.9979 - val_loss: 1.1926e-07 - val_acc: 1.0000\n",
      "Epoch 41/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9972Epoch 00040: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0076 - acc: 0.9972 - val_loss: 1.2092e-07 - val_acc: 1.0000\n",
      "Epoch 42/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9977Epoch 00041: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0057 - acc: 0.9977 - val_loss: 1.2175e-07 - val_acc: 1.0000\n",
      "Epoch 43/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9980Epoch 00042: loss improved from 0.00555 to 0.00538, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0054 - acc: 0.9980 - val_loss: 1.2642e-07 - val_acc: 1.0000\n",
      "Epoch 44/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9981Epoch 00043: loss improved from 0.00538 to 0.00501, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0050 - acc: 0.9981 - val_loss: 1.5879e-07 - val_acc: 1.0000\n",
      "Epoch 45/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9972Epoch 00044: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0069 - acc: 0.9971 - val_loss: 1.2402e-07 - val_acc: 1.0000\n",
      "Epoch 46/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9987Epoch 00045: loss improved from 0.00501 to 0.00443, saving model to ./logs/weights.best_deepAndHeavy_DataAugm.hdf5\n",
      "15970/15970 [==============================] - 9s - loss: 0.0044 - acc: 0.9987 - val_loss: 1.2046e-07 - val_acc: 1.0000\n",
      "Epoch 47/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9978Epoch 00046: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0057 - acc: 0.9978 - val_loss: 1.1939e-07 - val_acc: 1.0000\n",
      "Epoch 48/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9981Epoch 00047: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0055 - acc: 0.9981 - val_loss: 1.1930e-07 - val_acc: 1.0000\n",
      "Epoch 49/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9982Epoch 00048: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0052 - acc: 0.9982 - val_loss: 1.2254e-07 - val_acc: 1.0000\n",
      "Epoch 50/50\n",
      "15872/15970 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9975Epoch 00049: loss did not improve\n",
      "15970/15970 [==============================] - 9s - loss: 0.0062 - acc: 0.9976 - val_loss: 1.1949e-07 - val_acc: 1.0000\n",
      "Test loss:  1.22998132841e-07\n",
      "Test accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Activation\n",
    "from keras.models import Sequential, save_model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters= 16, kernel_size = (3,3), activation='relu', padding='same', input_shape= x_train.shape[1:]))\n",
    "model.add(Conv2D(filters= 16, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size= (2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters= 32, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(filters= 32, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size= (2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters= 64, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(filters= 64, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# configure the model\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# prints a summary representation of the model\n",
    "model.summary()\n",
    "\n",
    "# set tensorboard\n",
    "from keras.callbacks import TensorBoard\n",
    "tbCallback = TensorBoard(log_dir='./tensorboard/deepAndHeavy_DataAugm')\n",
    "tbCallback.set_model(model)\n",
    "\n",
    "# save the model\n",
    "save_model(model, './logs/my_model_deepAndHeavy_DataAugm.hdf5')\n",
    "\n",
    "# make parameters checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(filepath=\"./logs/weights.best_deepAndHeavy_DataAugm.hdf5\", verbose=1, \n",
    "                               monitor='loss', save_best_only=True, mode='auto')\n",
    "\n",
    "# # set early stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=False, # previously shuffled\n",
    "                validation_data=(x_valid, y_valid),\n",
    "                callbacks = [tbCallback, checkpoint, early_stopping])\n",
    "\n",
    "'''\n",
    "learning is done at this point\n",
    "'''\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss: ', score[0])\n",
    "print('Test accuracy: ', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                156850    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 229,152\n",
      "Trainable params: 229,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "incorrect:  0 , total:  1290  accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# restore trained model\n",
    "loaded_model = load_model('./logs/my_model_deepAndHeavy_DataAugm.hdf5')\n",
    "loaded_model.load_weights('./logs/weights.best_deepAndHeavy_DataAugm.hdf5')\n",
    "\n",
    "# prints a summary representation of the model\n",
    "loaded_model.summary()\n",
    "\n",
    "# predict test data\n",
    "predict = loaded_model.predict(x_test)\n",
    "\n",
    "# show error cases\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "cnt = 0\n",
    "for i in range(test_size):\n",
    "    if not (np.argmax(predict[i]) == np.argmax(y_test[i])):\n",
    "        print('file: %s' % filepath_list[shuffle_indices[i]])\n",
    "        print(np.argmax(predict[i]))\n",
    "        cnt += 1\n",
    "\n",
    "        img = mpimg.imread(filepath_list[shuffle_indices[i]])\n",
    "\n",
    "        plt.gray()\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "print('incorrect: ', cnt, ', total: ', test_size, ' accuracy: ', (test_size-cnt)/test_size)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
