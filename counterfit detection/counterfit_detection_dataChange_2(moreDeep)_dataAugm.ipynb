{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. data preprocessing 및 dataset 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "_A\n",
      "__BLUE\n",
      "..........\n",
      "__GREEN\n",
      "..........\n",
      "__IRR\n",
      "..........\n",
      "__IRT\n",
      ".....\n",
      "__RED\n",
      "..........\n",
      "_B\n",
      "__BLUE\n",
      "..........\n",
      "__GREEN\n",
      "..........\n",
      "__IRR\n",
      "..........\n",
      "__IRT\n",
      ".....\n",
      "__RED\n",
      "..........\n",
      "_C\n",
      "__BLUE\n",
      "..........\n",
      "__GREEN\n",
      "..........\n",
      "__IRR\n",
      "..........\n",
      "__IRT\n",
      ".....\n",
      "__RED\n",
      "..........\n",
      "_D\n",
      "__BLUE\n",
      "..........\n",
      "__GREEN\n",
      "..........\n",
      "__IRR\n",
      "..........\n",
      "__IRT\n",
      ".....\n",
      "__RED\n",
      "..........\n",
      "1\n",
      "_A\n",
      "__BLUE\n",
      "..........\n",
      "__GREEN\n",
      "..........\n",
      "__IRR\n",
      "..........\n",
      "__IRT\n",
      ".....\n",
      "__RED\n",
      "..........\n",
      "_B\n",
      "__BLUE\n",
      "..........\n",
      "__GREEN\n",
      "..........\n",
      "__IRR\n",
      "..........\n",
      "__IRT\n",
      ".....\n",
      "__RED\n",
      "..........\n",
      "_C\n",
      "__BLUE\n",
      "..........\n",
      "__GREEN\n",
      "..........\n",
      "__IRR\n",
      "..........\n",
      "__IRT\n",
      ".....\n",
      "__RED\n",
      "..........\n",
      "_D\n",
      "__BLUE\n",
      "..........\n",
      "__GREEN\n",
      "..........\n",
      "__IRR\n",
      "..........\n",
      "__IRT\n",
      ".....\n",
      "__RED\n",
      "..........\n",
      "data Len : 40\n",
      "(train + validation + test = total)\n",
      "28 + 6 + 6 = 40\n",
      "FRONT\n",
      "X train : (28, 65, 300, 5)\n",
      "Y train : (28, 2)\n",
      "X test  : (6, 65, 300, 5)\n",
      "X test  : (6, 2)\n",
      "BACK\n",
      "X train : (28, 65, 300, 5)\n",
      "Y train : (28, 2)\n",
      "X test  : (6, 65, 300, 5)\n",
      "X test  : (6, 2)\n",
      "40\n",
      "(65, 300, 5)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28, 65, 300, 5) (5 channels).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a09a1c8d5076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0my_trainAugm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtmp_type\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtmp_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0mx_augm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_augm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtmp_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtmp_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mcnt\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mx_trainAugm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtmp_type\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_trainAugm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtmp_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_augm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mflow\u001b[0;34m(self, x, y, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0msave_to_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_to_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0msave_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             save_format=save_format)\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     def flow_from_directory(self, directory,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, image_data_generator, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format)\u001b[0m\n\u001b[1;32m    775\u001b[0m                              \u001b[1;34m'either 1, 3 or 4 channels on axis '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannels_axis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                              \u001b[1;34m'However, it was passed an array with shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                              ' (' + str(self.x.shape[channels_axis]) + ' channels).')\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (28, 65, 300, 5) (5 channels)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import cv2\n",
    "# show error cases\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "\n",
    "num_classes = 2\n",
    "dataset_dir = \"../dataset\"\n",
    "labelName_list = [\"0\", \"1\"]\n",
    "channelName_list = [\"BLUE\", \"GREEN\", \"IRR\", \"IRT\", \"RED\"]\n",
    "headingName_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "flagToType_Dic = {\"A\" : [0, 1], \"B\" : [0, 1], \"C\" : [1, 0], \"D\" : [1, 0]}\n",
    "typeName_list = [\"FRONT\", \"BACK\"]\n",
    "\n",
    "def shuffle_data(data, label):\n",
    "    idx = np.arange(len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    shuffled_data = np.array(data)[np.array(idx)]\n",
    "    shuffled_label = np.array(label)[np.array(idx)]\n",
    "    \n",
    "    return shuffled_data, shuffled_label, idx\n",
    "\n",
    "def shuffle_data_in_defined(data, label, idx):\n",
    "    shuffled_data = np.array(data)[np.array(idx)]\n",
    "    shuffled_label = np.array(label)[np.array(idx)]\n",
    "    \n",
    "    return shuffled_data, shuffled_label, idx\n",
    "    \n",
    "def load_data(parent_dir, ext):\n",
    "    \n",
    "    image_Front_list = []\n",
    "    image_Back_list = []\n",
    "    label_Front_list = []\n",
    "    label_Back_list = []\n",
    "    fileFrontPath_list = []\n",
    "    fileBackPath_list = []\n",
    "    \n",
    "    image_Dic = {'FRONT' : image_Front_list, 'BACK' : image_Back_list}\n",
    "    label_Dic = {'FRONT' : label_Front_list, 'BACK' : label_Back_list}\n",
    "    filePath_Dic = {'FRONT' : fileFrontPath_list, 'BACK' : fileBackPath_list}\n",
    "\n",
    "    \n",
    "    labelNames = os.listdir(parent_dir)\n",
    "    for labelName in labelNames: # 하위 디렉토리 탐색      (label 폴더 : 0, 1)\n",
    "        if not(any(labelName in tmpstr for tmpstr in labelName_list)):\n",
    "            print(\"head ERR!!!\")\n",
    "            continue  # 해당 폴더가 투입면 폴더가 아닌 경우 다시 검색\n",
    "        labelDirPath = os.path.join(parent_dir, labelName)\n",
    "        print(labelName)\n",
    "        if os.path.isdir(labelDirPath): # 디렉토리인 경우\n",
    "            headingNames = os.listdir(labelDirPath)\n",
    "            for headingName in headingNames: # 하위 디렉토리 탐색      (heading 폴더 : A, B, C, D)\n",
    "                if not(any(headingName in tmpstr for tmpstr in headingName_list)):\n",
    "                    print(\"head ERR!!!\")\n",
    "                    continue  # 해당 폴더가 투입면 폴더가 아닌 경우 다시 검색\n",
    "                print('_' + headingName)\n",
    "                headingDirPath = os.path.join(labelDirPath, headingName)\n",
    "\n",
    "                if os.path.isdir(headingDirPath): # 디렉토리인 경우\n",
    "                    channelNames = os.listdir(headingDirPath)\n",
    "                    \n",
    "                    #로드 리스트 생성\n",
    "                    loadingImg_list = []\n",
    "                    loadinglable_list = []\n",
    "                    loadingName_list = []\n",
    "                    \n",
    "                    for channelName in channelNames:  # 하위 디렉토리 탐색      (각 Channel 접근 : R, G, B, IRR, IRT)\n",
    "                        if not(any(channelName in tmpstr for tmpstr in channelName_list)):\n",
    "                            print(\"channel ERR!!!\")\n",
    "                            continue  # 해당 폴더가 채널폴더가 아닌 경우 다시 검색\n",
    "                        print('__' + channelName)\n",
    "                        fileDirPath = os.path.join(headingDirPath, channelName)\n",
    "                        \n",
    "                        if os.path.isdir(fileDirPath): # 디렉토리인 경우\n",
    "                            fileNames = os.listdir(fileDirPath)\n",
    "                            for fileName in fileNames: #해당폴더 전체 이미지 탐색\n",
    "                                # 순서대로 새로운 채널에 입력\n",
    "                                filePath = os.path.join(fileDirPath, fileName)\n",
    "\n",
    "                                _ext = os.path.splitext(filePath)[-1] # 파일 확장자 확인\n",
    "                                if _ext == ext:\n",
    "                                    print('.', end = '')\n",
    "                                    image = mpimg.imread(filePath)\n",
    "                                    if image is None:\n",
    "                                        continue\n",
    "                                \n",
    "                                #이미지 크기 정규화\n",
    "                                image = image.astype(np.float32) / 255.\n",
    "                                #image = image.reshape(65, 308)\n",
    "                                \n",
    "                                #있는지 확인\n",
    "                                if any(fileName in tmpstr for tmpstr in loadingName_list): #파일이 있는지 없는지 확인\n",
    "                                    #있으면 dstack\n",
    "                                    tmpidx = loadingName_list.index(fileName)\n",
    "                                    loadingImg_list[tmpidx] = np.dstack((loadingImg_list[tmpidx], image))\n",
    "                                    shapetmp = loadingImg_list[tmpidx].shape\n",
    "                                    \n",
    "                                    if channelName == \"IRT\":\n",
    "                                        nameListLen = int(len(loadingName_list)/2 -1)\n",
    "                                        #print(type(nameListLen))\n",
    "                                        #print(loadingName_list)\n",
    "                                        for chatchName in loadingName_list[nameListLen:] :\n",
    "                                            if fileName[5:] in chatchName:\n",
    "                                                findedName = chatchName\n",
    "                                        tmp = loadingName_list.index(findedName)\n",
    "                                        loadingImg_list[tmp] = np.dstack((loadingImg_list[tmp], image))\n",
    "                                    \n",
    "                                    #if tmp == 0:\n",
    "                                        #print(\"!\" + fileName + \"\\t\" + str(tmp) + \"\\t\" + str(len(loadingImg_list)) + \"\\t\" +  str(image.shape) + \"\\t\" +  str(shapetmp) + \"\\t\" +  str(loadingImg_list[tmp].shape))\n",
    "                                    #print(\"!\" + fileName + \"\\t\" + str(len(loadingImg_list)) + \"\\t\" +  str(image.shape) + \"\\t\" +  str(shapetmp))\n",
    "                                else:\n",
    "                                    #없으면 append\n",
    "                                    loadingImg_list.append(image)\n",
    "                                    #label & name 추가\n",
    "                                    #loadinglable_list.append(str(len(loadingName_list))+\"\\t\"+labelName + headingName + fileName)\n",
    "                                    loadinglable_list.append(labelName)\n",
    "                                    loadingName_list.append(fileName)\n",
    "                                    #print(\"?\" + fileName)\n",
    "                            print()\n",
    "\n",
    "                    #리스트에 로드된 애들 입력\n",
    "                    flag = flagToType_Dic[headingName]\n",
    "                    typeSize = int(len(typeName_list))\n",
    "                    tmpDataSize = int(len(loadingImg_list)/typeSize)\n",
    "                    cnt = 0\n",
    "\n",
    "                    for tmp in flag:\n",
    "                        tmpstr = typeName_list[tmp]\n",
    "                        if labelName == labelName_list[0]:\n",
    "                            image_Dic[tmpstr] = image_Dic[tmpstr] + loadingImg_list[:tmpDataSize]\n",
    "                            label_Dic[tmpstr] = label_Dic[tmpstr] + loadinglable_list[:tmpDataSize]\n",
    "                        else :\n",
    "                            image_Dic[tmpstr] = image_Dic[tmpstr] + loadingImg_list[:tmpDataSize]\n",
    "                            label_Dic[tmpstr] = label_Dic[tmpstr] + loadinglable_list[:tmpDataSize]\n",
    "                        for i in range(int(len(loadingName_list)/typeSize)):\n",
    "                            filePath_Dic[tmpstr].append(os.path.join(headingName, loadingName_list[i + (cnt * int(len(loadingName_list)/typeSize))]))\n",
    "                            #print(tmpstr + \"\\t\" + filePath_Dic[tmpstr][i + int(len(image_Dic[tmpstr]) - tmpDataSize)])\n",
    "                            #print(tmpstr + \"\\t\" + str(i + (cnt * int(len(loadingName_list)/typeSize))) + \"\\t\" + str(len(filePath_Dic[tmpstr])))\n",
    "                        cnt += 1\n",
    "                        #print(\"!\" + tmpstr + \"\\t\" + str(len(image_Dic[tmpstr])) + \"\\t\" + str(len(loadingImg_list)))\n",
    "                    #print(len(image_Dic[headingName]))\n",
    "    return image_Dic, label_Dic, filePath_Dic\n",
    "\n",
    "\n",
    "# data 불러오기\n",
    "image_Dic, label_Dic, filePath_Dic = load_data(dataset_dir, '.bmp')\n",
    "\n",
    "# type 종류 마다 개수 확인\n",
    "data_len = len(label_Dic[typeName_list[0]])\n",
    "for tmp_type in typeName_list:\n",
    "    if data_len != len(label_Dic[tmp_type]):\n",
    "        print(\"DataSize ERR!!!\")\n",
    "        exit()\n",
    "\n",
    "# dataset 분할\n",
    "train_size = int(0.7 * data_len)\n",
    "valid_size = int(0.15 * data_len)\n",
    "test_size = data_len - train_size - valid_size\n",
    "print(\"data Len : \" + str(data_len))\n",
    "print(\"(train + validation + test = total)\")\n",
    "print(train_size, '+', valid_size, '+', test_size, '=', train_size+valid_size+test_size)\n",
    "\n",
    "# data shuffling\n",
    "shuffled_image_Dic = {}\n",
    "shuffled_label_Dic = {}\n",
    "shuffled_idx_Dic  = {}\n",
    "\n",
    "for tmp_type in typeName_list:\n",
    "    if tmp_type == typeName_list[0] :\n",
    "        shuffled_image_Dic[tmp_type], shuffled_label_Dic[tmp_type], shuffled_idx_Dic[tmp_type] = shuffle_data(image_Dic[tmp_type], label_Dic[tmp_type])\n",
    "    else :\n",
    "        shuffled_image_Dic[tmp_type], shuffled_label_Dic[tmp_type], shuffled_idx_Dic[tmp_type] = shuffle_data_in_defined(image_Dic[tmp_type], label_Dic[tmp_type], shuffled_idx_Dic[typeName_list[0]])\n",
    "\n",
    "# one-hot vector로 변환\n",
    "for tmp_type in typeName_list:\n",
    "    shuffled_label_Dic[tmp_type] = np_utils.to_categorical(shuffled_label_Dic[tmp_type], num_classes)\n",
    "\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "x_valid = {}\n",
    "y_valid = {}\n",
    "x_test = {}\n",
    "y_test = {}\n",
    "shuffle_indices = {} # 디버그용\n",
    "    \n",
    "# dataset 구축\n",
    "for tmp_type in typeName_list:\n",
    "    x_train[tmp_type] = shuffled_image_Dic[tmp_type][:train_size]\n",
    "    y_train[tmp_type] = shuffled_label_Dic[tmp_type][:train_size]\n",
    "    x_valid[tmp_type] = shuffled_image_Dic[tmp_type][train_size:data_len-test_size]\n",
    "    y_valid[tmp_type] = shuffled_label_Dic[tmp_type][train_size:data_len-test_size]\n",
    "    x_test[tmp_type] = shuffled_image_Dic[tmp_type][data_len-test_size:]\n",
    "    y_test[tmp_type] = shuffled_label_Dic[tmp_type][data_len-test_size:]\n",
    "    shuffle_indices = shuffled_idx_Dic[tmp_type][data_len-test_size:] # 디버그용\n",
    "    print(tmp_type)\n",
    "    print(\"X train : \" + str(x_train[tmp_type].shape))\n",
    "    print(\"Y train : \" + str(y_train[tmp_type].shape))\n",
    "    print(\"X test  : \" + str(x_test[tmp_type].shape))\n",
    "    print(\"X test  : \" + str(y_test[tmp_type].shape))\n",
    "\"\"\"\n",
    "#셔플 안한거 출력 결과\n",
    "for tmpType in typeName_list:\n",
    "    print(tmpType)\n",
    "    for dataSize in range(len(label_Dic[tmpType])):\n",
    "        print(\"_\\t\" + str(filePath_Dic[tmpType][dataSize]) + \"\\t\" + str(label_Dic[tmpType][dataSize]))  \n",
    "\"\"\"\n",
    "\n",
    "print(len(image_Dic[typeName_list[0]]))\n",
    "print(image_Dic[typeName_list[0]][0].shape)\n",
    "#print(image_Dic['A'])\n",
    "#img = image_Dic['A'][0].reshape(325, 308, 1)\n",
    "img = np.zeros((64, 304))\n",
    "#print(img.shape)\n",
    "#for i in range(len(filepath_list)):\n",
    "#    print(filepath_list[i])\n",
    "\n",
    "\"\"\"\n",
    "for chan in range(5):\n",
    "    for i in range(64):\n",
    "        for j in range(304):\n",
    "            img[i][j] = image_Dic['A'][5][i][j][chan]\n",
    "    plt.gray()\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "print()\n",
    "for tmp_type in typeName_list:\n",
    "    print(tmp_type)\n",
    "    for imgSize in range(len(shuffled_label_Dic[typeName_list[0]])):\n",
    "        print(str(imgSize) + \"\\t\" + str(shuffled_idx_Dic[tmp_type][imgSize]) + \"\\t\" + str(shuffled_label_Dic[tmp_type][imgSize]) + \"\\t=\\t\" + str(label_Dic[tmp_type][shuffled_idx_Dic[tmp_type][imgSize]]))\n",
    "\"\"\"\n",
    "print()\n",
    "\n",
    "### Start about Image Data Augmented ###\n",
    "\n",
    "# define data preparation\n",
    "x_trainAugm = {}\n",
    "y_trainAugm = {}\n",
    "\n",
    "datagen = image.ImageDataGenerator(width_shift_range=0.05, height_shift_range=0.05, horizontal_flip = True, vertical_flip = True, fill_mode='constant', cval=0)\n",
    "\n",
    "for tmp_type in typeName_list:\n",
    "    # fit parameters from data\n",
    "    x_trainAugm[tmp_type] = np.empty((0,) + x_train[tmp_type].shape[1:])\n",
    "    y_trainAugm[tmp_type] = np.empty((0,) + y_train[tmp_type].shape[1:])\n",
    "    cnt = 0\n",
    "    for x_augm, y_augm in datagen.flow(x_train[tmp_type], y_train[tmp_type], batch_size = 100):\n",
    "        cnt+=1\n",
    "        x_trainAugm[tmp_type] = np.concatenate((x_trainAugm[tmp_type], x_augm))\n",
    "        y_trainAugm[tmp_type] = np.concatenate((y_trainAugm[tmp_type], y_augm))\n",
    "        \n",
    "        if cnt%10==0:\n",
    "            print('')\n",
    "            print(str(cnt)+' :',end=' ')\n",
    "        else:\n",
    "            print('.',end='')\n",
    "         \n",
    "        if cnt > 20:\n",
    "            break\n",
    "\n",
    "    print(x_trainAugm[tmp_type].shape)\n",
    "    print(y_trainAugm[tmp_type].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tmp_type in typeName_list:\n",
    "    print(x_trainAugm[tmp_type].shape)\n",
    "    print(y_trainAugm[tmp_type].shape)\n",
    "\n",
    "    x_train[tmp_type] = x_trainAugm[tmp_type]\n",
    "    y_train[tmp_type] = y_trainAugm[tmp_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Activation\n",
    "from keras.models import Sequential, save_model\n",
    "\n",
    "\n",
    "for tmpType in typeName_list:\n",
    "    path_each_str = \"/\" + tmpType + \"/\"\n",
    "    projectName_str = \"1_full_dataChange_ver2\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters= 16, kernel_size = (3,3), activation='relu', padding='valid', input_shape= x_train[tmpType].shape[1:]))\n",
    "    model.add(Conv2D(filters= 16, kernel_size = (3,3), activation='relu', padding='valid'))\n",
    "    model.add(MaxPooling2D(pool_size= (2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters= 32, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(filters= 32, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size= (2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(filters= 64, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(filters= 64, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size= (2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(filters= 64, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(filters= 64, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size= (2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(filters= 32, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(filters= 32, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size= (2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(filters= 32, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(filters= 32, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # configure the model\n",
    "    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # prints a summary representation of the model\n",
    "    model.summary()\n",
    "\n",
    "    # set tensorboard\n",
    "    from keras.callbacks import TensorBoard\n",
    "    tbCallback = TensorBoard(log_dir=('./tensorboard/' + path_each_str + projectName_str))\n",
    "    tbCallback.set_model(model)\n",
    "\n",
    "    # save the model\n",
    "    if not os.path.exists('./logs' + path_each_str):\n",
    "        os.makedirs('./logs' + path_each_str)\n",
    "    save_model(model, './logs' + path_each_str + 'my_model_' + projectName_str + '.hdf5')\n",
    "\n",
    "    # make parameters checkpoint\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    checkpoint = ModelCheckpoint(filepath=(\"./logs/\" + path_each_str + \"weights.best_\" + projectName_str + \".hdf5\"), verbose=1, \n",
    "                                   monitor='loss', save_best_only=True, mode='auto')\n",
    "\n",
    "    # # set early stopping\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # train the model\n",
    "    model.fit(x_train[tmpType], y_train[tmpType],\n",
    "                    epochs=50,\n",
    "                    batch_size=256,\n",
    "                    shuffle=False, # previously shuffled\n",
    "                    validation_data=(x_valid[tmpType], y_valid[tmpType]),\n",
    "                    callbacks = [tbCallback, checkpoint, early_stopping])\n",
    "\n",
    "    '''\n",
    "    learning is done at this point\n",
    "    '''\n",
    "    score = model.evaluate(x_test[tmpType], y_test[tmpType], verbose=0)\n",
    "\n",
    "    print('Test loss: ', score[0])\n",
    "    print('Test accuracy: ', score[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
